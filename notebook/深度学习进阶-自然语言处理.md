# 深度学习进阶：自然语言处理

# 1. NN复习

PyTorch中神经网络层的一些共同特征:

- 所有的层都有 forward() 方法和 backward() 方法
  - 正向传播
  - 反向传播
- 所有的层都有 params 和 grads 实例变量
  - 保存权重和偏置等参数
  - 参数对应的形式，保存梯度

神经网络的学习

- 损失函数
- 导数和梯度
- 链式法则
- 计算图
- 梯度推导，反向传播，权重更新(SGD, AdaGrad, Adam)

```python
class MatMul:
    def __init__(self, W):
        self.params = [W]
        self.grads = [np.zeros_like(W)]
        self.x = None

    def forward(self, x):
        W, = self.params
        out = np.dot(x, W)
        self.x = x
        return out

    def backward(self, dout):
        W, = self.params
        dx = np.dot(dout, W.T)
        dW = np.dot(self.x.T, dout)
        self.grads[0][...] = dW  # deep copy
        return dx

class Sigmoid:
    def __init__(self):
        self.params, self.grads = [], []
        self.out = None

    def forward(self, x):
        out = 1 / (1 + np.exp(-x))
        self.out = out
        return out

    def backward(self, dout):
        dx = dout * (1.0 - self.out) * self.out
        return dx
```

Affine Layer:

![image-20241016171753621](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241016171753621.png)

```python
class Affine:
    def __init__(self, W, b):
        self.params = [W, b]
        self.grads = [np.zeros_like(W), np.zeros_like(b)]
        self.x = None

    def forward(self, x):
        W, b = self.params
        out = np.dot(x, W) + b
        self.x = x
        return out

    def backward(self, dout):
        W, b = self.params
        dx = np.dot(dout, W.T)
        dW = np.dot(self.x.T, dout)
        db = np.sum(dout, axis=0)

        self.grads[0][...] = dW
        self.grads[1][...] = db
        return dx
```

# 2. 古典NLP处理方法

- 基于同义词词典的方法
  - WordNet from NLTK Lib
- 基于计数(统计)的方法
  - Corpus as basis
  - 共现矩阵+余弦相似度
- 基于计数的方法的改进：
  - 点互信息 $PMI(x,y)=\log_2\frac{P(x,y)}{P(x)P(y)}$ , PMI越高相关性越强
  - 正点互信息 $PPMI(x,y)=max(0,PMI(x,y))$
  - 降维：奇异值分解 SVD, $X=USV^T$
- 语料库：Penn Treebank Corpus (PTB)

# 3. word2vec

>  基于推理的方法：Word2Vec；虽然和基于统计的方法相差很大，但是两者的背景都是分布式假设。
>
> Paper: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)

两种方法的相同点：基于分布式假设（单词含义由其周围的单词构成）

两种方法的区别：

- 基于计数的方法一次性处理全部学习数据
- 基于推理的方法使用部分学习数据逐步学习

![image-20241020021053415](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241020021053415-1729361454162-1.png)

Word2Vec 中使用得两种神经网络：

- Continuous bag-of-words (CBOW): 通过上下文词预测目标词
- Skip-gram: 通过目标词预测上下文词

## CBOW

![image-20241020233921756](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241020233921756.png)

![image-20241020234124432](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241020234124432.png)

推理：

![image-20241021012319181](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241021012319181.png)

学习：

- CBOW 模型只是学习语料库中单词的出现模式
- Softmax + CEE 模型进行学习
- word2vec 常用输入侧的权重 $W_{in}$
- GloVe 同时使用两个权重（相加）

![image-20241021012415523](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241021012415523.png)

![image-20241021012559818](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241021012559818.png)

误差反向传播：

![image-20241021013001101](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241021013001101.png)

## CBOW中的数学

联合概率 $P(A,B)$, 后验概率 $P(A|B)$, Window_size=1

CBOW模型的数学表示：
$$
P(w_t|w_{t-1},w_{t+1})
$$
损失函数(负对数似然 negative log likelihood)：
$$
L=-\log P(w_t|w_{t-1},w_{t+1})
$$
损失函数扩展为所有语料库：
$$
L=-\frac{1}{T}\sum_{t=1}^{T}\log P(w_t|w_{t-1},w_{t+1})
$$

## Skip-gram Model

![image-20241021013647141](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241021013647141.png)

![image-20241021013704693](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241021013704693.png)

模型数学公式：
$$
P(w_{t-1},w_{t+1}|w_t)=P(w_{t-1}|w_t)P(w_{t+1}|w_t)
$$
损失函数：
$$
L = -\frac{1}{T} \sum_{t=1}^{T} \left( \log P(w_{t-1} \mid w_t) + \log P(w_{t+1} \mid w_t) \right)
$$

# 4. word2vec 加速

改进：

- Embedding 层的加入
- 损失函数使用 Negative Sampling

词汇量巨大的时候会出现性能瓶颈：

- 输入层的  one-hot  表示和权重矩阵  $W_{in}$  的乘积: One-hot 的原因
- 中间层和权重矩阵 $W_{out}$ 的乘积以及  Softmax  层的计算: 导致 Softmax 层的计算量增加

![image-20241021014332310](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241021014332310.png)

## Embedding

NLP中，单词的密集向量表示称为：

- 词嵌入 word embedding
- 词的分布式表示 distributed representation (基于NN)，distributional representation (基于统计)

![image-20241021020829739](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241021020829739.png)

## Negative Sampling

使用 Negative Sampling 替代 Softmax: 使计算量保持较低或恒定

CBOW模型：

- $W_{out}$ 的矩阵乘法计算量很大
- Softmax 层计算量很大

![image-20241021020950144](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241021020950144.png)

负采样的重点：用二分类拟合多分类

改进：

- $W_{out}$ 中保持了各个单词ID对应的单词向量
- 提取对应单词向量，再求内积
- Sigmoid 函数输出对应单词的概率

![image-20241021021743925](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241021021743925.png)

![image-20241021021750793](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241021021750793.png)

![image-20241021021814373](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241021021814373.png)

![image-20241021021900894](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241021021900894.png)

**改进后的模型全貌**：

![image-20241021022222794](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241021022222794.png)

**负采样**：

- 选择若干个负例，学习到概率接近0
- 基于语料库的统计数据，进行采样的方法比随机抽样要好

![image-20241021022410245](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241021022410245.png)

![image-20241021022648084](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241021022648084.png)

## NLP 问题处理流程

![image-20241021023038406](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241021023038406.png)

![image-20241021023050923](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241021023050923.png)

# 5. RNN

## 概率和语言模型

Language Model: 输出单词序列发生的概率 $P(w_t|w_{1},\cdots,w_{t-1})$

使用后验概率将**联合概率**分解：
$$
P(w_1, \ldots, w_m) = P(w_m \mid w_1, \ldots, w_{m-1}) P(w_{m-1} \mid w_1, \ldots, w_{m-2}) \cdots P(w_3 \mid w_1, w_2) P(w_2 \mid w_1) P(w_1) \\
= \prod_{t=1}^{m} P(w_t \mid w_1, \ldots, w_{t-1})
$$
**似然函数**用于衡量给定参数下观测数据的可能性。

**后验概率**是指在给定观测数据的情况下，某个事件发生的概率。它可以通过贝叶斯定理计算，公式为：
$$
P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}
$$

其中：
- $P(A \mid B)$ 是后验概率，即在已知 \( B \) 发生的情况下 \( A \) 发生的概率。
- $P(B \mid A)$ 是似然函数，即在 \( A \) 发生的情况下 \( B \) 发生的概率。
- $P(A)$ 是先验概率，即在没有任何观测数据时，事件 \( A \) 的概率。
- $P(B)$ 是观测数据的边缘概率。

![image-20241021024834459](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241021024834459.png)

**马尔可夫链|马尔可夫性**：系统的未来状态只依赖于当前状态，而与过去状态无关

CBOW模型强行用作LM，限制上下文的大小在某个近似值，并且中间层通过把单词序列向量拼接组成

## RNN

![image-20241024003548213](./%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.assets/image-20241024003548213.png)
$$
h_t = \tanh(h_{t-1}W_h + x_tW_x + b)
$$
