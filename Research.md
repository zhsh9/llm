## Model

| å¹´ä»½ | åå­—                                                         | ç®€ä»‹                                                         | ç²¾è¯»                              |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | --------------------------------- |
| 2013 | [Word2Vec](https://arxiv.org/abs/1301.3781)                  | å¼€å¯NLPåŸºäºç¥ç»ç½‘ç»œçš„æ¨ç†æ—¶ä»£                                |                                   |
| 2017 | [Transformer](https://arxiv.org/abs/1706.03762)              | ç»§ MLPã€CNNã€RNN åçš„ç¬¬å››å¤§ç±»æ¶æ„                            | [here](./research/transformer.md) |
| 2018 | [ELMo](https://arxiv.org/abs/1802.05365)                     | ä½¿ç”¨é¢„è®­ç»ƒçš„åŒå‘è¯­è¨€æ¨¡å‹(biLM)çš„å†…éƒ¨çŠ¶æ€æ¥å­¦ä¹ è¯å‘é‡         |                                   |
| 2018 | [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) | ä½¿ç”¨ Transformer è§£ç å™¨æ¥åšé¢„è®­ç»ƒ                            | [here](./research/gpt.md)         |
| 2018 | [BERT](https://arxiv.org/abs/1810.04805)                     | ä½¿ç”¨ Transformer ç¼–ç å™¨æ¥åšé¢„è®­ç»ƒï¼ŒTransformer ä¸€ç»Ÿ NLP çš„å¼€å§‹ | [here](./research/bert.md)        |
| 2019 | [T5](https://arxiv.org/pdf/1910.10683)                       | ä½¿ç”¨ Transformer è§£ç å™¨å’Œç¼–ç å™¨ï¼Œæ–‡æœ¬åˆ°æ–‡æœ¬æ ¼å¼çš„é¢„è®­ç»ƒ      |                                   |
| 2019 | [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) | æ›´å¤§çš„ GPT æ¨¡å‹ï¼Œæœç€ zero-shot learning è¿ˆäº†ä¸€å¤§æ­¥          | [here](./research/gpt.md)         |
| 2020 | [GPT-3](https://arxiv.org/abs/2005.14165)                    | 100 å€æ›´å¤§çš„ GPT-2ï¼Œfew-shot learning æ•ˆæœæ˜¾è‘—               | [here](./research/gpt.md)         |
| 2022 | [InstructGPT](https://arxiv.org/abs/2203.02155)              | ä½¿ç”¨äººç±»åé¦ˆå¯¹GPT-3è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒ                              |                                   |
| 2022 | [Claude](https://arxiv.org/abs/2204.05862)                   | Anthropic LLM (Claude) æ›´æ³¨é‡ç¨³å®šå’Œå®‰å…¨çš„å…¬å¸æˆ–æˆä¸ºOpenAIæœ€å¤§å¯¹æ‰‹ |                                   |
| 2022 | [ChatGPT](https://openai.com/index/chatgpt/)                 | åŸºäºInstructGPTçš„å¯¹è¯å¼AIåŠ©æ‰‹                                |                                   |
| 2023 | [GPT-4](https://cdn.openai.com/papers/gpt-4.pdf)             | We used pythonğŸ˜‚ å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œæ”¯æŒå›¾ç‰‡å’Œæ–‡æœ¬çš„è¾“å…¥ï¼Œæ–‡æœ¬çš„è¾“å‡º | here                              |
| 2023 | [Llama](https://arxiv.org/abs/2302.13971)                    | Metaå¼€æºLLMå‘é—­æºå¤§æ¨¡å‹å‘å‡ºå†²é”‹å·è§’ï¼Œå‚æ•°é‡7Båˆ°65B           |                                   |
| 2023 | [Llama 2](https://arxiv.org/abs/2307.09288)                  | 70Båˆ°70Bå‚æ•°é‡å¼€æºå¤§æ¨¡å‹å¯èƒ½æˆä¸ºé—­æºå¤§æ¨¡å‹æ›¿ä»£å“             |                                   |
| 2024 | [Llama 3](https://arxiv.org/pdf/2407.21783)                  | å¼ºå¤§çš„ Meta å¼€æºæ¨¡å‹ - åŠ¨æ€æ‰©å±•ï¼Œå¤šæ¨¡æ€å­¦ä¹ ï¼Œé›¶æ ·æœ¬å­¦ä¹ ï¼Œé«˜æ•ˆè®¡ç®— |                                   |

## Literature Review & Evaluation

| å¹´ä»½ | åå­—                                                         | ç®€ä»‹                                                         | ç²¾è¯»                       |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | -------------------------- |
| 2020 | [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732) | Transformer æ¨¡å‹çš„å…¨é¢ç»¼è¿°                                   |                            |
| 2022 | [*On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258) | LLM å¿…çœ‹ç»¼è¿°: LLM èƒ½åŠ›ã€æœºé‡å’ŒæŒ‘æˆ˜ï¼Œå‚ç›´é¢†åŸŸåº”ç”¨ï¼Œå½±å“       | todo                       |
| 2023 | [*A Survey of Large Language Models](https://arxiv.org/abs/2303.18223) | LLM å¿…çœ‹ç»¼è¿°: èµ„æºã€é¢„è®­ç»ƒã€å¾®è°ƒã€åº”ç”¨ã€èƒ½åŠ›ç­‰               | todo                       |
| 2023 | [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond](https://arxiv.org/abs/2304.13712) | æ¢è®¨LLMåœ¨å„ç§NLPä»»åŠ¡ä¸­çš„åº”ç”¨ã€ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œæ¶µç›–äº†æ¨¡å‹ã€æ•°æ®å’Œä¸‹æ¸¸ä»»åŠ¡ç­‰å¤šä¸ªè§’åº¦ |                            |
| 2023 | [Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models](https://arxiv.org/abs/2304.01852) | å¯¹ ChatGPT ç›¸å…³ç ”ç©¶è¿›è¡Œäº†å…¨é¢ç»¼è¿°                            |                            |
| 2023 | [*Holistic Evaluation of Language Models](https://arxiv.org/abs/2211.09110) | å…¨é¢çš„å¤§æ¨¡å‹è¯„æµ‹ç»¼è¿°                                         | [here](./research/helm.md) |
| 2023 | [Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712) | GPT-4è¯„æµ‹ï¼ŒAGIå…ƒå¹´çš„å·è§’ğŸ“¯                                    |                            |
| 2024 | [What is the Role of Small Models in the LLM Era: A Survey](https://arxiv.org/abs/2409.06857) | ä»åä½œå’Œç«äº‰å…³ç³»æ¥çœ‹ LLM å’Œ SM                               |                            |
| 2024 | [A Survey of Large Language Models for Graphs](https://arxiv.org/abs/2405.08011) | ç»¼è¿°ä¸åŒè®¾è®¡æ–¹æ³•æ¥æ•´åˆ LLMs å’Œå›¾å­¦ä¹ æŠ€æœ¯                     |                            |

## Principle

| å¹´ä»½ | åå­—                                                         | ç®€ä»‹                                                         | ç²¾è¯» |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---- |
| 2020 | [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) | ç ”ç©¶LMåœ¨CELä¸Šçš„ç»éªŒæ€§ç¼©æ”¾è§„å¾‹ï¼Œå‘ç°æŸå¤±ä¸æ¨¡å‹å¤§å°ã€æ•°æ®é›†å¤§å°å’Œè®­ç»ƒè®¡ç®—é‡å‘ˆå¹‚å¾‹å…³ç³»ï¼Œå¹¶æå‡ºäº†åœ¨æœ‰é™è®¡ç®—é¢„ç®—ä¸‹çš„æœ€ä½³èµ„æºåˆ†é…ç­–ç•¥ |      |
| 2021 | [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/abs/2112.11446) | é€šè¿‡è®­ç»ƒå’Œåˆ†æä»æ•°åƒä¸‡åˆ°2000äº¿å‚æ•°çš„Transformerè¯­è¨€æ¨¡å‹ï¼ˆåŒ…æ‹¬Gopherï¼‰ï¼Œå…¨é¢ç ”ç©¶äº†æ¨¡å‹è§„æ¨¡ä¸æ€§èƒ½çš„å…³ç³» |      |
| 2022 | [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) | ç ”ç©¶åœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹è®­ç»ƒtransformerè¯­è¨€æ¨¡å‹çš„æœ€ä¼˜æ¨¡å‹è§„æ¨¡å’Œè®­ç»ƒæ•°æ®é‡ |      |
| 2023 | [DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining)](https://arxiv.org/abs/2305.10429) | ç ”ç©¶å¤§è¯­è¨€æ¨¡å‹é¢†åŸŸé€‚åº”é¢„è®­ç»ƒ(D-CPT)ä¸­é€šç”¨è¯­æ–™ä¸é¢†åŸŸè¯­æ–™çš„æœ€ä½³æ··åˆæ¯”ä¾‹é—®é¢˜ |      |
| 2024 | [D-CPT Law: Domain-specific Continual Pre-Training Scaling Law for Large Language Models](https://arxiv.org/abs/2406.01375) | é€šè¿‡åœ¨å°å‹ä»£ç†æ¨¡å‹ä¸Šä¼˜åŒ–é¢„è®­ç»ƒæ•°æ®çš„ä¸åŒé¢†åŸŸæƒé‡ï¼Œå†å°†è¿™äº›æƒé‡è¿ç§»åˆ°å¤§æ¨¡å‹è®­ç»ƒä¸­ï¼Œåœ¨ä¸éœ€è¦ä¸‹æ¸¸ä»»åŠ¡ä¿¡æ¯çš„æƒ…å†µä¸‹å®ç°äº†æ€§èƒ½æå‡ |      |

## Small Language Model

| å¹´ä»½ | åå­—                                                         | ç®€ä»‹                                                         | ç²¾è¯» |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---- |
| 2024 | [MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies](https://arxiv.org/abs/2404.06395) | é€šè¿‡å¯æ‰©å±•çš„è®­ç»ƒç­–ç•¥ï¼Œæ¢ç´¢å°å‹è¯­è¨€æ¨¡å‹åœ¨èµ„æºæ•ˆç‡å’Œæ€§èƒ½ä¸Šçš„æ½œåŠ›ï¼Œä½œä¸ºå¤§æ¨¡å‹çš„é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆ |      |
| 2024 | [Small Language Models: Survey, Measurements, and Insights](https://arxiv.org/abs/2409.15790) | æ¥è‡ªåŒ—äº¬é‚®ç”µå¤§å­¦ã€é¹åŸå®éªŒå®¤ã€Helixon Researchã€å‰‘æ¡¥å¤§å­¦çš„å°å‹è¯­è¨€æ¨¡å‹SLMçš„ç ”ç©¶ç»¼è¿° |      |

## Tokenizer

| å¹´ä»½ | åå­—                                                         | ç®€ä»‹                                                         | ç²¾è¯» |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---- |
| 2016 | [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909) | BPE: è´ªå©ªçš„ã€åŸºäºé¢‘ç‡çš„ç¬¦å·æ›¿æ¢ç®—æ³•ï¼Œé€šè¿‡è¿­ä»£åœ°åˆå¹¶æœ€é¢‘ç¹å‡ºç°çš„ç›¸é‚»å­—ç¬¦å¯¹ï¼ˆæˆ–å­è¯å¯¹ï¼‰æ¥åˆ›å»ºæ–°çš„ç¬¦å· |      |
| 2018 | [SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing](https://arxiv.org/abs/1808.06226) | BPEå’ŒULM(Unigram LM)çš„æ”¹è¿›å’Œæ‰©å±•ï¼Œæä¾›ä¸€ç§è¯­è¨€æ— å…³çš„åˆ†è¯æ–¹æ³• |      |

## Optimization of Model Architecture

| å¹´ä»½ | åå­—                                                         | ç®€ä»‹                                                         | ç²¾è¯» |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---- |
| 2019 | [Understanding and Improving Layer Normalization](https://arxiv.org/abs/1911.07013) | æ·±å…¥åˆ†æå±‚å½’ä¸€åŒ–æœºåˆ¶ï¼Œå‘ç°å…¶æ•ˆæœä¸»è¦æºäºå‡å€¼å’Œæ–¹å·®çš„å¯¼æ•°è€Œéå‰å‘å½’ä¸€åŒ– |      |
| 2019 | [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509v1) | æå‡ºäº†ç¨€ç–å› å­åŒ–çš„æ³¨æ„åŠ›çŸ©é˜µå’Œå¿«é€Ÿæ³¨æ„åŠ›æ ¸ï¼Œä»¥é™ä½Transformeræ¨¡å‹åœ¨å¤„ç†é•¿åºåˆ—æ—¶çš„è®¡ç®—å¤æ‚åº¦ |      |
| 2021 | [Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth](https://arxiv.org/abs/2103.03404) | çº¯æ³¨æ„åŠ›æ¨¡å‹çš„è¾“å‡ºéšæ·±åº¦å‘ˆåŒæŒ‡æ•°çº§é€€åŒ–ä¸ºç§©ä¸º1çš„çŸ©é˜µï¼Œè€Œè·³è·ƒè¿æ¥å’Œå¤šå±‚æ„ŸçŸ¥å™¨å¯ä»¥é˜²æ­¢è¿™ç§é€€åŒ– |      |
| 2022 | [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://proceedings.neurips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html) | FlashAttentionæ˜¯ä¸€ç§æ–°çš„æ³¨æ„åŠ›è®¡ç®—æ–¹æ³•ï¼Œé€šè¿‡IOæ„ŸçŸ¥è®¾è®¡æ˜¾è‘—æé«˜äº†Transformeræ¨¡å‹åœ¨é•¿åºåˆ—å¤„ç†æ—¶çš„è®¡ç®—æ•ˆç‡å’Œå†…å­˜ä½¿ç”¨ |      |
| 2023 | [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691) | é€šè¿‡æ”¹è¿›å¹¶è¡ŒåŒ–å’Œå·¥ä½œåˆ†åŒºç­–ç•¥æ¥è¿›ä¸€æ­¥æé«˜æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—æ•ˆç‡ |      |
| 2024 | [FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision](https://arxiv.org/abs/2407.08608) | è¿‡å¼•å…¥å¼‚æ­¥è®¡ç®—å’Œä½ç²¾åº¦è¿ç®—æ¥è¿›ä¸€æ­¥æé«˜æ³¨æ„åŠ›æœºåˆ¶çš„é€Ÿåº¦å’Œå‡†ç¡®æ€§ |      |

## Quantization

| å¹´ä»½ | åå­—                                                                                                  | ç®€ä»‹                                                                                                          | ç²¾è¯» |
| ---- | ----------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------- | ---- |
| 2024 | [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764) | å¾®è½¯ç ”ç©¶é™¢æå‡ºçš„ä¸€ä¸ªæ¨¡å‹æ¶æ„ï¼Œé‡‡ç”¨æç«¯é‡å­é‡åŒ–ï¼Œä»…ç”¨-1, 0, 1è¡¨ç¤ºæ¯ä¸ªå‚æ•°ï¼Œæ¯ä¸ªå‚æ•°ä»…ä½¿ç”¨ $log_2(3)=1.58$ æ¯”ç‰¹ | todo |

## Fine-Tuning

| å¹´ä»½ | åå­—                                                         | ç®€ä»‹                                                         | ç²¾è¯» |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---- |
| 2019 | [Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593) | ä»äººç±»åå¥½å‡ºå‘ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œå¥–åŠ±æœºåˆ¶æ¥å¾®è°ƒè¯­è¨€æ¨¡å‹ï¼Œä»…ä»…ä½¿ç”¨5000ä¸ªäººå·¥è¯„ä¼°æ ·æœ¬ |      |
| 2021 | [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) | PERFå‚æ•°åŒ–é«˜æ•ˆå¾®è°ƒ: Low rank, LoRA                           |      |

## Reinforcement Learning

| å¹´ä»½ | åå­—                                                                                                    | ç®€ä»‹                                                                     | ç²¾è¯» |
| ---- | ------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ | ---- |
| 2017 | [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)                             | å¼ºåŒ–å­¦ä¹ å¿…è¯»ç®—æ³•ï¼Œé€šè¿‡ç¯å¢ƒäº¤äº’é‡‡æ ·å’Œä¼˜åŒ–æ›¿ä»£ç›®æ ‡å‡½æ•°ï¼Œå®ç°å¤šè½®å°æ‰¹é‡æ›´æ–° |      |
| 2020 | [Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325)                           | å¼€è¾Ÿè¿›è¡ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–æ¨¡å‹è®­ç»ƒï¼Œé€šè¿‡æ‘˜è¦ä»»åŠ¡å±•ç¤º                     |      |
| 2023 | [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) | InstructGPT: é€šè¿‡äººç±»åé¦ˆæ¥å¾®è°ƒè¯­è¨€æ¨¡å‹                                  |      |

## RAG

| å¹´ä»½ | åå­—                                                                                                                                                               | ç®€ä»‹                                                               | ç²¾è¯» |
| ---- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------ | ---- |
| 2020 | [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)                                                               | RAG å¼€å±±ä¹‹ä½œ: ç»“åˆé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œå¤–éƒ¨çŸ¥è¯†æ£€ç´¢ï¼Œè§£å†³çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ | todo |
| 2023 | [Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997)                                                             | å¯¹RAGåœ¨LLMä¸­çš„åº”ç”¨è¿›è¡Œå…¨é¢ç»¼è¿°                                     |      |
| 2024 | [Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely](https://arxiv.org/abs/2409.14924) | å¾®è½¯å‡ºå“ç»¼è¿°: è¯¦ç»†æ¢è®¨äº†ä½¿ç”¨æ›´æœ‰æ•ˆçš„æŠ€æœ¯å°†å¤–éƒ¨æ•°æ®é›†æˆåˆ°LLMs       |      |

## Chain of Thought

| å¹´ä»½ | åå­—                                                                                                          | ç®€ä»‹                                                                                                                                                                               | ç²¾è¯» |
| ---- | ------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- |
| 2023 | [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)     | æ¢ç´¢æ€ç»´é“¾æç¤ºæŠ€æœ¯(CoT)å¦‚ä½•æ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†èƒ½åŠ›                                                                                                                      |      |
| 2023 | [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)                             | å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬æ¨ç†èƒ½åŠ›å’Œæ½œåŠ›                                                                                                                                         |      |
| 2023 | [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)   | æå‡ºäº†ä¸€ç§åä¸º"è‡ªä¸€è‡´æ€§"çš„æ–°è§£ç ç­–ç•¥ï¼Œé€šè¿‡é‡‡æ ·å¤šæ¡æ¨ç†è·¯å¾„å¹¶é€‰æ‹©æœ€ä¸€è‡´çš„ç­”æ¡ˆ                                                                                                       |      |
| 2024 | [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](https://arxiv.org/abs/2403.09629) | å¼€è¾Ÿæ–°çš„è¯­è¨€æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡è®©æ¨¡å‹åœ¨ç”Ÿæˆæ¯ä¸ª token æ—¶å­¦ä¹ äº§ç”Ÿè§£é‡Šæ€§çš„å†…éƒ¨æ€è€ƒï¼Œæé«˜äº†æ¨¡å‹åœ¨é¢„æµ‹å›°éš¾ token å’Œå›ç­”å¤æ‚é—®é¢˜æ—¶çš„èƒ½åŠ›ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒå°±èƒ½å®ç°é›¶æ ·æœ¬æ€§èƒ½æå‡ |      |

## Code

| å¹´ä»½ | åå­—                                                                                 | ç®€ä»‹                                               | ç²¾è¯» |
| ---- | ------------------------------------------------------------------------------------ | -------------------------------------------------- | ---- |
| 2021 | [Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374) | å…¬å¼€ä»£ç å¾®è°ƒçš„ç¼–ç¨‹å¤§æ¨¡å‹ï¼Œæœ‰å±€é™ä¹Ÿæœ‰æœªæ¥å‘å±•       |      |
| 2022 | [Competition-Level Code Generation with AlphaCode](https://arxiv.org/abs/2203.07814) | å±•ç¤ºè§£å†³é«˜éš¾åº¦ç¼–ç¨‹é—®é¢˜çš„æ½œåŠ›ï¼Œèƒ½ç”Ÿæˆç«èµ›çº§åˆ«çš„ä»£ç  |      |
| 2024 | [Qwen2.5-Coder Technical Report](https://huggingface.co/papers/2409.12186)           | é˜¿é‡ŒåŒä¹‰åƒé—® Qwen2.5-Coder æŠ€æœ¯æŠ¥å‘Š                |      |

## AI Agent

| å¹´ä»½ | åå­—                                                                                              | ç®€ä»‹                                                           | ç²¾è¯» |
| ---- | ------------------------------------------------------------------------------------------------- | -------------------------------------------------------------- | ---- |
| 2023 | [Toolformer: Language models can teach themselves to use tools](https://arxiv.org/abs/2302.04761) | é€šè¿‡è‡ªæˆ‘å­¦ä¹ ä½¿ç”¨å¤–éƒ¨å·¥å…·ï¼Œå…‹æœå¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŸºç¡€åŠŸèƒ½ä¸Šçš„å±€é™æ€§ |      |

## RWKV

RWKV (Receptance Weighted Key Value)

| å¹´ä»½ | åå­— | ç®€ä»‹ | ç²¾è¯» |
| ---- | ---- | ---- | ---- |
|      |      |      |      |

## Mamba

| å¹´ä»½ | åå­—                                                         | ç®€ä»‹                                     | ç²¾è¯» |
| ---- | ------------------------------------------------------------ | ---------------------------------------- | ---- |
| 2023 | [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752) | Transformerå·²"æ­»"ï¼ŒMambaå½“ç«‹             |      |
| 2024 | [Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](https://arxiv.org/abs/2405.21060) | Mamba2æå‡ºï¼Œæ ¸å¿ƒå±‚å¯¹Mambaé€‰æ‹©æ€§SMMçš„æ”¹è¿› |      |
