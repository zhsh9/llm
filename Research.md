## Model

| å¹´ä»½ | åå­—                                                         | ç®€ä»‹                                                         | ç²¾è¯»                              |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | --------------------------------- |
| 2017 | [Transformer](https://arxiv.org/abs/1706.03762)              | ç»§ MLPã€CNNã€RNN åçš„ç¬¬å››å¤§ç±»æ¶æ„                            | [here](./research/transformer.md) |
| 2018 | [ELMo](https://arxiv.org/abs/1802.05365)                     | ä½¿ç”¨é¢„è®­ç»ƒçš„åŒå‘è¯­è¨€æ¨¡å‹(biLM)çš„å†…éƒ¨çŠ¶æ€æ¥å­¦ä¹ è¯å‘é‡         | here                              |
| 2018 | [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) | ä½¿ç”¨ Transformer è§£ç å™¨æ¥åšé¢„è®­ç»ƒ                            | here                              |
| 2018 | [BERT](https://arxiv.org/abs/1810.04805)                     | ä½¿ç”¨ Transformer ç¼–ç å™¨æ¥åšé¢„è®­ç»ƒï¼ŒTransformer ä¸€ç»Ÿ NLP çš„å¼€å§‹ | [here](./research/bert.md)        |
| 2019 | [T5](https://arxiv.org/pdf/1910.10683)                       | ä½¿ç”¨ Transformer è§£ç å™¨å’Œç¼–ç å™¨ï¼Œæ–‡æœ¬åˆ°æ–‡æœ¬æ ¼å¼çš„é¢„è®­ç»ƒ      |                                   |
| 2019 | [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) | æ›´å¤§çš„ GPT æ¨¡å‹ï¼Œæœç€ zero-shot learning è¿ˆäº†ä¸€å¤§æ­¥          | here                              |
| 2020 | [GPT-3](https://arxiv.org/abs/2005.14165)                    | 100 å€æ›´å¤§çš„ GPT-2ï¼Œfew-shot learning æ•ˆæœæ˜¾è‘—               | here                              |
| 2022 | [InstructGPT](https://arxiv.org/abs/2203.02155)              | ä½¿ç”¨äººç±»åé¦ˆå¯¹GPT-3è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒ                              |                                   |
| 2022 | [ChatGPT](https://openai.com/index/chatgpt/)                 | åŸºäºInstructGPTçš„å¯¹è¯å¼AIåŠ©æ‰‹                                |                                   |
| 2023 | [GPT-4](https://cdn.openai.com/papers/gpt-4.pdf)             | We used pythonğŸ˜‚ å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œæ”¯æŒå›¾ç‰‡å’Œæ–‡æœ¬çš„è¾“å…¥ï¼Œæ–‡æœ¬çš„è¾“å‡º | here                              |
| 2023 | [Llama](https://arxiv.org/abs/2302.13971)                    | Metaå¼€æºLLMå‘é—­æºå¤§æ¨¡å‹å‘å‡ºå†²é”‹å·è§’ï¼Œå‚æ•°é‡7Båˆ°65B           |                                   |
| 2023 | [Llama 2](https://arxiv.org/abs/2307.09288)                  | 70Båˆ°70Bå‚æ•°é‡å¼€æºå¤§æ¨¡å‹å¯èƒ½æˆä¸ºé—­æºå¤§æ¨¡å‹æ›¿ä»£å“             |                                   |
| 2024 | [Llama 3.1](https://arxiv.org/pdf/2407.21783)                | å¼ºå¤§çš„ Meta å¼€æºæ¨¡å‹ - åŠ¨æ€æ‰©å±•ï¼Œå¤šæ¨¡æ€å­¦ä¹ ï¼Œé›¶æ ·æœ¬å­¦ä¹ ï¼Œé«˜æ•ˆè®¡ç®— |                                   |

## Literature Review

| å¹´ä»½ | åå­—                                                         | ç®€ä»‹                                                   | ç²¾è¯» |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------ | ---- |
| 2020 | [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732) | Transformer æ¨¡å‹çš„å…¨é¢ç»¼è¿°                             | here |
| 2022 | `*` [On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258) | LLM å¿…çœ‹ç»¼è¿°: LLM èƒ½åŠ›ã€æœºé‡å’ŒæŒ‘æˆ˜ï¼Œå‚ç›´é¢†åŸŸåº”ç”¨ï¼Œå½±å“ |      |
| 2023 | `*` [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223) | LLM å¿…çœ‹ç»¼è¿°: èµ„æºã€é¢„è®­ç»ƒã€å¾®è°ƒã€åº”ç”¨ã€èƒ½åŠ›ç­‰         | here |
| 2023 | [Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models](https://arxiv.org/abs/2304.01852) | å¯¹ ChatGPT ç›¸å…³ç ”ç©¶è¿›è¡Œäº†å…¨é¢ç»¼è¿°                      |      |
| 2023 | [Holistic Evaluation of Language Models](https://arxiv.org/abs/2211.09110) | å…¨é¢çš„å¤§æ¨¡å‹è¯„æµ‹ç»¼è¿°                                   |      |
| 2024 | [What is the Role of Small Models in the LLM Era: A Survey](https://arxiv.org/abs/2409.06857) | ä»åä½œå’Œç«äº‰å…³ç³»æ¥çœ‹ LLM å’Œ SM                         |      |
| 2024 | [A Survey of Large Language Models for Graphs](https://arxiv.org/abs/2405.08011) | ç»¼è¿°ä¸åŒè®¾è®¡æ–¹æ³•æ¥æ•´åˆ LLMs å’Œå›¾å­¦ä¹ æŠ€æœ¯               |      |

## Optimization of Model Architecture

| å¹´ä»½ | åå­—                                                                                                                         | ç®€ä»‹                                                                                        | ç²¾è¯» |
| ---- | ---------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- | ---- |
| 2019 | [Understanding and Improving Layer Normalization](https://arxiv.org/abs/1911.07013)                                          | æ·±å…¥åˆ†æå±‚å½’ä¸€åŒ–æœºåˆ¶ï¼Œå‘ç°å…¶æ•ˆæœä¸»è¦æºäºå‡å€¼å’Œæ–¹å·®çš„å¯¼æ•°è€Œéå‰å‘å½’ä¸€åŒ–                      ||
| 2021 | [Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth](https://arxiv.org/abs/2103.03404) | çº¯æ³¨æ„åŠ›æ¨¡å‹çš„è¾“å‡ºéšæ·±åº¦å‘ˆåŒæŒ‡æ•°çº§é€€åŒ–ä¸ºç§©ä¸º1çš„çŸ©é˜µï¼Œè€Œè·³è·ƒè¿æ¥å’Œå¤šå±‚æ„ŸçŸ¥å™¨å¯ä»¥é˜²æ­¢è¿™ç§é€€åŒ– |      |
| 2022 | [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://proceedings.neurips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html) | FlashAttentionæ˜¯ä¸€ç§æ–°çš„æ³¨æ„åŠ›è®¡ç®—æ–¹æ³•ï¼Œé€šè¿‡IOæ„ŸçŸ¥è®¾è®¡æ˜¾è‘—æé«˜äº†Transformeræ¨¡å‹åœ¨é•¿åºåˆ—å¤„ç†æ—¶çš„è®¡ç®—æ•ˆç‡å’Œå†…å­˜ä½¿ç”¨ | |
| 2023 | [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691) | é€šè¿‡æ”¹è¿›å¹¶è¡ŒåŒ–å’Œå·¥ä½œåˆ†åŒºç­–ç•¥æ¥è¿›ä¸€æ­¥æé«˜æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—æ•ˆç‡ | |
| 2024 | [FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision](https://arxiv.org/abs/2407.08608) | è¿‡å¼•å…¥å¼‚æ­¥è®¡ç®—å’Œä½ç²¾åº¦è¿ç®—æ¥è¿›ä¸€æ­¥æé«˜æ³¨æ„åŠ›æœºåˆ¶çš„é€Ÿåº¦å’Œå‡†ç¡®æ€§ | |

## Quantization

| å¹´ä»½ | åå­—                                                         | ç®€ä»‹                                                         | ç²¾è¯» |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---- |
| 2024 | [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764) | å¾®è½¯ç ”ç©¶é™¢æå‡ºçš„ä¸€ä¸ªæ¨¡å‹æ¶æ„ï¼Œé‡‡ç”¨æç«¯é‡å­é‡åŒ–ï¼Œä»…ç”¨-1, 0, 1è¡¨ç¤ºæ¯ä¸ªå‚æ•°ï¼Œæ¯ä¸ªå‚æ•°ä»…ä½¿ç”¨ $log_2(3)=1.58$ æ¯”ç‰¹ |      |

## Fine-Tuning

| å¹´ä»½ | åå­—                                                         | ç®€ä»‹                               | ç²¾è¯» |
| ---- | ------------------------------------------------------------ | ---------------------------------- | ---- |
| 2021 | [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) | PERFå‚æ•°åŒ–é«˜æ•ˆå¾®è°ƒ: Low rank, LoRA |      |

## RAG

| å¹´ä»½ | åå­—                                                         | ç®€ä»‹                                                         | ç²¾è¯» |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---- |
| 2020 | [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) | RAG å¼€å±±ä¹‹ä½œ: ç»“åˆé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œå¤–éƒ¨çŸ¥è¯†æ£€ç´¢ï¼Œè§£å†³çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ | here |
| 2023 | [Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997) | å¯¹RAGåœ¨LLMä¸­çš„åº”ç”¨è¿›è¡Œå…¨é¢ç»¼è¿°                               |      |
| 2024 | [Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely](https://arxiv.org/abs/2409.14924) | å¾®è½¯å‡ºå“ç»¼è¿°: è¯¦ç»†æ¢è®¨äº†ä½¿ç”¨æ›´æœ‰æ•ˆçš„æŠ€æœ¯å°†å¤–éƒ¨æ•°æ®é›†æˆåˆ°LLMs |      |

## Reinforcement Learning

| å¹´ä»½ | åå­—                                                         | ç®€ä»‹                                                         | ç²¾è¯» |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---- |
| 2017 | [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347) | å¼ºåŒ–å­¦ä¹ å¿…è¯»ç®—æ³•ï¼Œé€šè¿‡ç¯å¢ƒäº¤äº’é‡‡æ ·å’Œä¼˜åŒ–æ›¿ä»£ç›®æ ‡å‡½æ•°ï¼Œå®ç°å¤šè½®å°æ‰¹é‡æ›´æ–° |      |
| 2020 | [Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325) | å¼€è¾Ÿè¿›è¡ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–æ¨¡å‹è®­ç»ƒï¼Œé€šè¿‡æ‘˜è¦ä»»åŠ¡å±•ç¤º         |      |
| 2023 | [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) | InstructGPT: é€šè¿‡äººç±»åé¦ˆæ¥å¾®è°ƒè¯­è¨€æ¨¡å‹                      |      |

## Chain of Thought

| å¹´ä»½ | åå­—                                                                                                          | ç®€ä»‹                                                                                                                                                                               | ç²¾è¯» |
| ---- | ------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- |
| 2023 | [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)     | æ¢ç´¢æ€ç»´é“¾æç¤ºæŠ€æœ¯(CoT)å¦‚ä½•æ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†èƒ½åŠ›                                                                                                                      |      |
| 2023 | [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)                             | å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬æ¨ç†èƒ½åŠ›å’Œæ½œåŠ›                                                                                                                                         |      |
| 2023 | [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)   | æå‡ºäº†ä¸€ç§åä¸º"è‡ªä¸€è‡´æ€§"çš„æ–°è§£ç ç­–ç•¥ï¼Œé€šè¿‡é‡‡æ ·å¤šæ¡æ¨ç†è·¯å¾„å¹¶é€‰æ‹©æœ€ä¸€è‡´çš„ç­”æ¡ˆ                                                                                                       |      |
| 2024 | [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](https://arxiv.org/abs/2403.09629) | å¼€è¾Ÿæ–°çš„è¯­è¨€æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡è®©æ¨¡å‹åœ¨ç”Ÿæˆæ¯ä¸ª token æ—¶å­¦ä¹ äº§ç”Ÿè§£é‡Šæ€§çš„å†…éƒ¨æ€è€ƒï¼Œæé«˜äº†æ¨¡å‹åœ¨é¢„æµ‹å›°éš¾ token å’Œå›ç­”å¤æ‚é—®é¢˜æ—¶çš„èƒ½åŠ›ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒå°±èƒ½å®ç°é›¶æ ·æœ¬æ€§èƒ½æå‡ |      |

## Code

| å¹´ä»½ | åå­—                                                         | ç®€ä»‹                                               | ç²¾è¯» |
| ---- | ------------------------------------------------------------ | -------------------------------------------------- | ---- |
| 2021 | [Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374) | å…¬å¼€ä»£ç å¾®è°ƒçš„ç¼–ç¨‹å¤§æ¨¡å‹ï¼Œæœ‰å±€é™ä¹Ÿæœ‰æœªæ¥å‘å±•       |      |
| 2022 | [Competition-Level Code Generation with AlphaCode](https://arxiv.org/abs/2203.07814) | å±•ç¤ºè§£å†³é«˜éš¾åº¦ç¼–ç¨‹é—®é¢˜çš„æ½œåŠ›ï¼Œèƒ½ç”Ÿæˆç«èµ›çº§åˆ«çš„ä»£ç  |      |
| 2024 | [Qwen2.5-Coder Technical Report](https://huggingface.co/papers/2409.12186) | é˜¿é‡ŒåŒä¹‰åƒé—® Qwen2.5-Coder æŠ€æœ¯æŠ¥å‘Š                |      |

## RWKV

RWKV (Receptance Weighted Key Value)

| å¹´ä»½ | åå­— | ç®€ä»‹ | ç²¾è¯» |
| ---- | ---- | ---- | ---- |
|      |      |      |      |

## Mamba

| å¹´ä»½ | åå­— | ç®€ä»‹ | ç²¾è¯» |
| ---- | ---- | ---- | ---- |
|      |      |      |      |
