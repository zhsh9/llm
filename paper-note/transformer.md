# [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

## Key Concept

- tokenization
- input embedding
- position encoding
- residual
- q, k, v
- add & layer norm
- encoder & decoder
- attention & self-attention
- multi head attention
- mask attention
- encoder & decoder attention
- output probability & logit & softmax

## Reference
