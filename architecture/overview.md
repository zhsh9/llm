从训练方式和模型类型来看，LLM 有以下几种分类：

- CausalLM 因果语言模型: GPT
  - 特点：单向注意力，从左到右顺序生成文本
  - 原理：模型只能看到当前位置之前的tokens，预测下一个token
  - 应用：主要用于文本生成任务
- MaskLM 掩码语言模型: BERT
  - 特点：双向注意力，可以利用上下文的全部信息
  - 原理：随机遮蔽输入文本中的一些tokens，训练模型预测这些被遮蔽的tokens
  - 应用：主要用于理解任务，如文本分类、命名实体识别等
- PrefixLM 前缀语言模型: GLM, UniLM
  - 特点：在同一个模型中结合了双向和单向注意力机制
  - 原理：结合了CausalLM和MLM的特点，对输入的前缀部分进行双向编码，对后缀部分进行单向解码
  - 应用：可用于各种理解和生成任务

从编码器-解码器的角度来看，LLM 有以下几种架构：

- Encoder-only: BERT, RoBERTa
  - 特点: 擅长理解和表示输入文本，适合文本分类、命名实体识别等
  - 原理: 对输入序列进行双向编码，不生成文本
- Decoder-only: GPT-n, Llama
  - 特点: 擅长生成连贯的文本，适合文本生成、对话等
  - 原理: 通过自回归方式训练&预测&生成输出
- Encoder-Decoder: T5, BART
  - 特点: 结合编码器和解码器，适合各种 NLP 任务
  - 原理: 编码器处理文本输入，解码器基于编码信息生成输出

Q:  
为什么现在的 LLM 都是 Decoder Only 的架构？