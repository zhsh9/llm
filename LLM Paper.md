## Model

| å¹´ä»½ | åå­—                                                                                                                           | ç®€ä»‹                                                            | ç²¾è¯»                                |
| ---- | ------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------- | ----------------------------------- |
| 2017 | [Transformer](https://arxiv.org/abs/1706.03762)                                                                                | ç»§MLPã€CNNã€RNNåçš„ç¬¬å››å¤§ç±»æ¶æ„                                 | [here](./paper-note/transformer.md) |
| 2018 | [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) | ä½¿ç”¨ Transformer è§£ç å™¨æ¥åšé¢„è®­ç»ƒ                               |                                     |
| 2018 | [BERT](https://arxiv.org/abs/1810.04805)                                                                                       | ä½¿ç”¨ Transformer ç¼–ç å™¨æ¥åšé¢„è®­ç»ƒï¼ŒTransformerä¸€ç»ŸNLPçš„å¼€å§‹     |                                     |
| 2019 | [T5](https://arxiv.org/pdf/1910.10683)                                                                                         | ä½¿ç”¨ Transformer è§£ç å™¨å’Œç¼–ç å™¨ï¼Œæ–‡æœ¬åˆ°æ–‡æœ¬æ ¼å¼çš„é¢„è®­ç»ƒ         |                                     |
| 2019 | [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)    | æ›´å¤§çš„ GPT æ¨¡å‹ï¼Œæœç€zero-shot learningè¿ˆäº†ä¸€å¤§æ­¥               |                                     |
| 2020 | [GPT-3](https://arxiv.org/abs/2005.14165)                                                                                      | 100å€æ›´å¤§çš„ GPT-2ï¼Œfew-shot learningæ•ˆæœæ˜¾è‘—                    |                                     |
| 2023 | [GPT-4](https://cdn.openai.com/papers/gpt-4.pdf)                                                                               | We used pythonğŸ˜‚ å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œæ”¯æŒå›¾ç‰‡å’Œæ–‡æœ¬çš„è¾“å…¥ï¼Œæ–‡æœ¬çš„è¾“å‡º  |                                     |
| 2024 | [Llama 3.1](https://arxiv.org/pdf/2407.21783)                                                                                  | å¼ºå¤§çš„Metaå¼€æºæ¨¡å‹ - åŠ¨æ€æ‰©å±•ï¼Œå¤šæ¨¡æ€å­¦ä¹ ï¼Œé›¶æ ·æœ¬å­¦ä¹ ï¼Œé«˜æ•ˆè®¡ç®— |                                     |

## Comprehension

| å¹´ä»½ | åå­—                                                                                                                                | ç®€ä»‹                                          | ç²¾è¯» |
| ---- | ----------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------- | ---- |
| 2020 | [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)                                                                | X-former æ¨¡å‹çš„å…¨é¢ç»¼è¿°                       |      |
| 2022 | [On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258)                                             | LLMèƒ½åŠ›ã€æœºé‡å’ŒæŒ‘æˆ˜ï¼Œå‚ç›´é¢†åŸŸåº”ç”¨ï¼Œå½±å“       |      |
| 2023 | [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)                                                               | LLMå¿…çœ‹ç»¼è¿°: èµ„æºã€é¢„è®­ç»ƒã€å¾®è°ƒã€åº”ç”¨ã€èƒ½åŠ›ç­‰ |      |
| 2023 | [Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models](https://arxiv.org/abs/2304.01852) | å¯¹ChatGPTç›¸å…³ç ”ç©¶è¿›è¡Œäº†å…¨é¢ç»¼è¿°               |      |
| 2024 | [What is the Role of Small Models in the LLM Era: A Survey](https://arxiv.org/abs/2409.06857)                                       | ä»åä½œå’Œç«äº‰å…³ç³»æ¥çœ‹LLMå’ŒSM                   |      |
| 2024 | [A Survey of Large Language Models for Graphs](https://arxiv.org/abs/2405.08011)                                                    | ç»¼è¿°ä¸åŒè®¾è®¡æ–¹æ³•æ¥æ•´åˆLLMså’Œå›¾å­¦ä¹ æŠ€æœ¯        |      |

## RL & CoT

| å¹´ä»½ | åå­—                                                                                                          | ç®€æ´                                                                                                                                                                           | ç²¾è¯» |
| ---- | ------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---- |
| 2020 | [Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325)                                 | å¼€è¾Ÿè¿›è¡ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–æ¨¡å‹è®­ç»ƒï¼Œé€šè¿‡æ‘˜è¦ä»»åŠ¡å±•ç¤º                                                                                                                           |      |
| 2024 | [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](https://arxiv.org/abs/2403.09629) | å¼€è¾Ÿæ–°çš„è¯­è¨€æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡è®©æ¨¡å‹åœ¨ç”Ÿæˆæ¯ä¸ªtokenæ—¶å­¦ä¹ äº§ç”Ÿè§£é‡Šæ€§çš„å†…éƒ¨æ€è€ƒï¼Œæé«˜äº†æ¨¡å‹åœ¨é¢„æµ‹å›°éš¾tokenå’Œå›ç­”å¤æ‚é—®é¢˜æ—¶çš„èƒ½åŠ›ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒå°±èƒ½å®ç°é›¶æ ·æœ¬æ€§èƒ½æå‡ |      |

