## Model

| 年份 | 名字                                                                                                                           | 简介                                                            | 精读                                |
| ---- | ------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------- | ----------------------------------- |
| 2017 | [Transformer](https://arxiv.org/abs/1706.03762)                                                                                | 继MLP、CNN、RNN后的第四大类架构                                 | [here](./paper-note/transformer.md) |
| 2018 | [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) | 使用 Transformer 解码器来做预训练                               |                                     |
| 2018 | [BERT](https://arxiv.org/abs/1810.04805)                                                                                       | 使用 Transformer 编码器来做预训练，Transformer一统NLP的开始     |                                     |
| 2019 | [T5](https://arxiv.org/pdf/1910.10683)                                                                                         | 使用 Transformer 解码器和编码器，文本到文本格式的预训练         |                                     |
| 2019 | [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)    | 更大的 GPT 模型，朝着zero-shot learning迈了一大步               |                                     |
| 2020 | [GPT-3](https://arxiv.org/abs/2005.14165)                                                                                      | 100倍更大的 GPT-2，few-shot learning效果显著                    |                                     |
| 2023 | [GPT-4](https://cdn.openai.com/papers/gpt-4.pdf)                                                                               | We used python😂 多模态大模型，支持图片和文本的输入，文本的输出  |                                     |
| 2024 | [Llama 3.1](https://arxiv.org/pdf/2407.21783)                                                                                  | 强大的Meta开源模型 - 动态扩展，多模态学习，零样本学习，高效计算 |                                     |

## Comprehension

| 年份 | 名字                                                                                                                                | 简介                                          | 精读 |
| ---- | ----------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------- | ---- |
| 2020 | [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)                                                                | X-former 模型的全面综述                       |      |
| 2022 | [On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258)                                             | LLM能力、机遇和挑战，垂直领域应用，影响       |      |
| 2023 | [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)                                                               | LLM必看综述: 资源、预训练、微调、应用、能力等 |      |
| 2023 | [Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models](https://arxiv.org/abs/2304.01852) | 对ChatGPT相关研究进行了全面综述               |      |
| 2024 | [What is the Role of Small Models in the LLM Era: A Survey](https://arxiv.org/abs/2409.06857)                                       | 从协作和竞争关系来看LLM和SM                   |      |
| 2024 | [A Survey of Large Language Models for Graphs](https://arxiv.org/abs/2405.08011)                                                    | 综述不同设计方法来整合LLMs和图学习技术        |      |

## RL & CoT

| 年份 | 名字                                                                                                          | 简洁                                                                                                                                                                           | 精读 |
| ---- | ------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---- |
| 2020 | [Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325)                                 | 开辟进行基于人类反馈的强化模型训练，通过摘要任务展示                                                                                                                           |      |
| 2024 | [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](https://arxiv.org/abs/2403.09629) | 开辟新的语言模型训练方法，通过让模型在生成每个token时学习产生解释性的内部思考，提高了模型在预测困难token和回答复杂问题时的能力，无需针对特定任务进行微调就能实现零样本性能提升 |      |

