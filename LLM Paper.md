| 年份 | 名字                                                                                                                           | 简介                                                            | 精读 |
| ---- | ------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------- | ---- |
| 2017 | [Transformer](https://arxiv.org/abs/1706.03762)                                                                                | 继MLP、CNN、RNN后的第四大类架构                                 |      |
| 2018 | [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) | 使用 Transformer 解码器来做预训练                               |      |
| 2018 | [BERT](https://arxiv.org/abs/1810.04805)                                                                                       | 使用 Transformer 编码器来做预训练，Transformer一统NLP的开始     |      |
| 2019 | [T5](https://arxiv.org/pdf/1910.10683)                                                                                         | 使用 Transformer 解码器和编码器，文本到文本格式的预训练         |      |
| 2019 | [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)    | 更大的 GPT 模型，朝着zero-shot learning迈了一大步               |      |
| 2020 | [GPT-3](https://arxiv.org/abs/2005.14165)                                                                                      | 100倍更大的 GPT-2，few-shot learning效果显著                    |      |
| 2023 | [GPT-4](https://cdn.openai.com/papers/gpt-4.pdf)                                                                               | We used python😂 多模态大模型，支持图片和文本的输入，文本的输出  |      |
| 2024 | [Llama 3.1](https://arxiv.org/pdf/2407.21783)                                                                                  | 强大的Meta开源模型 - 动态扩展，多模态学习，零样本学习，高效计算 |      |

