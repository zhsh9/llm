# 监督学习

## ML和监督学习概论

### What is ML

基础知识

- ML：基于数据，构建概率模型，运行模型对数据进行预测与分析
- 学习：系统，通过执行某个过程，改进其性能
- 特点：建立在计算机&网络上的，数据驱动(对象)，对数据进行预测和分析(目的)，交叉学科
- 数据：文本，图形，视频，音频
- 方法：有监督学习，无监督学习，强化学习(reinforcement learning)

ML 三要素  
- 模型 model
- 策略 strategy
- 算法 algorithm

ML 可以概括为  
- 从给定的、有限的、用于学习的训练数据集合出发
- 假设数据是独立同分布产生的(每个样本都是相互独立&遵循相同的概率分布)
- 假设要学习的模型属于某个函数的集合(即假设空间 hypothesis space)
- 应用某个评价标准(evaluation critetion)，从假设空间中选出一个最优模型，使其对已知训练数据&未知测试数据有最优预测
- 最优模型的选取由算法实现

ML 的研究  
- 方法：开发新的方法
- 理论：探索ML方法的有效性&效率&基本理论问题
- 应用：垂直领域解决问题

计算机科学的三维组成  
- 系统
- 计算
- 信息(ML here)

### 机器学习的基本分类

#### 有监督学习

- 从标注数据中学习预测模型
- 本质：学习输入到输出的映射的统计规律
- 概念：输入空间，输出空间，实例，特征向量，特征空间；训练数据，测试数据，样本
- 假设：输入和输出的随机变量遵循联合概率分布 $P(X,Y)$ 表示分布(密度)函数
  - 学习过程中，假设的联合概率分布虽然存在，但是学习系统是不知道具体定义的
  - 训练&测试数据被看作是联合概率分布独立同分布产生的
- 目的：学习一个最好的输入到输出的映射(即，模型)
- 假设空间：由输入空间到输出空间的映射的集合
- 模型可以是概率模型or非概率模型，由条件概率分布 $P(X,Y)$ or 决策函数 $Y=f(X)$ 表示
- 预测：对具体的输入 $P(y|x)$ or $y=f(x)$

监督学习的过程：学习，预测

![Supervised Learning](./机器学习方法.assets/1.png)

#### 无监督学习

- 从无标注数据中学习预测模型
- 本质：学习数据中的统计规律或潜在结构
- 输入：特征向量；输出：对输入的分析结果，由输入的类别、转换或概率表示
- 作用：对数据的聚类，降维，概率估计
- 假设 $X$ 是输入空间 $Z$ 是隐式结构空间，要学习的模型可以表示为 ( $x\in X$ , $z\in Z$ )
  - 函数 $z=g(x)$
  - 条件概率分布 $P(z|x)$
  - 条件概率分布 $P(x|z)$

无监督学习的过程：学习，预测

![Unsupervised Learning](./机器学习方法.assets/2.png)

#### 强化学习

- 在与环境的连续互动中学习最优行为策略
- 假设智能系统与环境的互动基于马尔可夫决策过程(Markov decision process)
- 智能系统能观测到的是与环境互动得到的数据序列
- 本质：学习最优的序贯决策
- 强化学习的MDP：状态、奖励、动作序列上的随机过程，由四元组 $\langle S,A,P,r \rangle$ 组成

![Reinforcement Learning](./机器学习方法.assets/3.png)

[!] 这里还需要更深入的理解。

#### 半监督学习 & 主动学习

半监督学习  
- 利用大量未标注数据中的信息，辅助少量标注数据，进行监督
- 解决人工标注成本问题

主动学习  
- 机器不断主动给出实例让教师进行标注
- 利用标注数据让学生学习预测模型的机器学习问题

### 按模型分类

#### 概率与非概率模型

- 概率模型(Probabilistic model)
  - 决策树
  - 朴素贝尔斯
  - 隐马尔可夫模型
  - 条件随机场
  - 概率潜在语义分析
  - 潜在迪利克雷分类
  - 高斯混合模型
- 非概率模型(Non-probabilistic model)
  - 感知机
  - 支持向量机
  - k近邻
  - AdaBoost
  - k均值
  - 潜在语义分析
  - 神经网络
- 逻辑斯谛回归(both)

主要区别  
- 内在结构
- 概率模型可以表示为联合概率分布的形式，变量表示输入、输出、隐变量、参数
- 非概率模型不存在这样的联合概率分布

- 有监督学习
  - 概率模型: $P(y|x)$
  - 非概率模型: $y=f(x)$
- 无监督学习
  - 概率模型: $P(z|x)$ or $P(x|z)$
  - 非概率模型: $y=g(x)$

概率模型的推理，随机变量( $x$, $y$ )  
- 加法规则: $P(x)=\sum_y P(x,y)$
- 乘法规则: $P(x,y)=P(x)P(y|x)$

线性与非线性模型  
- 线性模型：感知机，线性支持向量机，k近邻，k均值，潜在语义分析
- 非线性模型：核函数支持向量机，AdaBoost，神经网络

参数化与非参数化模型  
- Parametric Model: 模型参数的维度固定，可以由有限维度参数完全刻画
- Non-parametric Model: 模型参数的维度不固定(无穷大)，随着训练数据量增加而增加

### 按算法分类

- 在线学习(online learning): 每次接受一个样本
- 批量学习(batch learning): 每次接受批量样本

![Online Learning](./机器学习方法.assets/4.png)

### 按技巧分类

#### 贝叶斯学习

- 在概率模型的学习和推理中，利用贝叶斯定义，计算在给定数据条件下模型的条件概率（后验概率），应用这个原理进行模型的估计，对数据的预测
- 将模型、未观测要素及其参数用变量表示，使用模型的先验分布

假设随机变量 D 表示数据，随机变量 $\theta$ 表示模型参数，根据贝叶斯定理，用以下公式计算后验概率

$$P(\theta|D)=\frac{P(\theta)P(D|\theta)}{P(D)}$$

其中，

- $P(\theta)$ 先验概率
- $P(D|\theta)$ 似然函数

预测时，计算数据对后验概率分布的期望值：

$$P(x|D) = \int P(x|\theta, D)P(\theta|D)d\theta$$

贝叶斯估计和极大似然估计比较：

![Bayesian & Argmax](./机器学习方法.assets/5.png)

#### 核方法

Kernel method: 使用核函数表示和学习非线性模型
- 核方法可以把线性模型扩展到非线性模型学习
- 方法：显示地定义从输入空间（低维空间）到特征空间（高维空间）的映射，在特征空间中内积计算
- SVM：把输入空间的线性不可分问题转换为特征空间的线性可分问题
- 技巧：不显式定义映射，定义核函数

表示定理

假设 $x_1$ 和 $x_2$ 是输入空间的任意两个实例（向量），其内积是 $\langle x_1, x_2 \rangle$。假设从输入空间到特征空间的映射是 $\varphi$，那么 $x_1$ 和 $x_2$ 在特征空间的映像分别是 $\varphi(x_1)$ 和 $\varphi(x_2)$，它们的内积是 $\langle \varphi(x_1), \varphi(x_2) \rangle$。核方法直接在输入空间中定义核函数 $K(x_1, x_2)$，使其满足：

$$K(x_1, x_2) = \langle \varphi(x_1), \varphi(x_2) \rangle$$

这个表示定理给出了核函数成立的充要条件。核函数 $K(x_1, x_2)$ 实际上计算的是特征空间中两个向量的内积，但不需要显式地知道映射函数 $\varphi$ 的具体形式。

1. 避免了在高维特征空间中的直接计算
2. 允许在原始输入空间中进行计算，同时获得非线性变换的好处
3. 可以处理各种类型的数据，包括向量、字符串、图像等

### 机器学习方法三要素

$$方法 = 模型+策略+算法$$

#### 模型

函数集合定义:

- 决策函数

$$
F = \{f|Y=f_\theta(X), \theta \in R^n\}
$$

- 条件概率

$$
F = \{P|P_\theta(Y|X), \theta \in R^n\}
$$

#### 策略

预测的度量  
- 损失函数：度量模型一次预测的好坏
- 风险函数(期望损失)：度量模型平均意义下预测的好坏

经验风险|损失(empirical risk|loss)：模型关于训练数据集的平均损失

$$
R_emp(f) = \frac{1}{N} \sum_{i=1}^N L(y_i,f(x_i))
$$

结构风险|损失()：

$$
R_srm(f) = \frac{1}{N} \sum_{i=1}^N L(y_i,f(x_i)) + \lambda J(f)
$$

基本策略  
- 经验风险最小化：样本容量足够大时，有很好的学习效果 e.g. 极大似然估计(maximum likelihood estimation)
- 结构风险最小化：在经验风险的基础上表示模型复杂度的正则化项(regulation)or惩罚项(penalty term)，防止过拟合 e.g. 最大后验概率估计

#### 算法

- 机器学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，选择计算方法求解最优模型
- ML归结为最优化问题，ML算法为最优化问题的算法
- 重点问题：如何找到全局最优解，并且求解过程非常高效

### 模型评估与模型选择

- 训练误差(调参优化) & 测试误差(泛化能力)
- 过拟合：所选模型的复杂度比真模型更高

### 正则化

正则化：模型复杂度的单调递增函数，可以是模型参数向量( $w$ )的范数

- 奥卡姆剃刀原理(Occam's razor)：在所有可能选择的模型中，能很好解释已知数据并十分简单才是最好的模型
- 贝叶斯估计角度：正则化项=模型的先验概率；复杂模型先验概率小，简单模型先验概率大

模型参数向量的范数选择

- L1范数

$$\lambda \frac{||w||^2}{2}$$

- L2范数

$$\lambda||w||_1$$

### 交叉验证

- 如果样本充足，切分为 training dataset, validation, test
- 交叉验证，重复使用数据

S-折交叉验证(S-fold cross validation)  
- 随机将数据集切分为S个互不相交、大小相同的子集
- 利用S-1个子集数据训练，1个子集测试模型
- 重复这个过程，选出S次评测中平均测试误差最小的模型

### 泛化能力

方法的泛化能力指的是该方法学习到的模型对未知数据的预测能力

泛化误差的定义：如果学到的模型是 $\hat{f}$，那么用这个模型对未知数据预测的误差即为泛化误差（generalization error）：

$$R_{\exp}(\hat{f}) = E_P[L(Y, \hat{f}(X))]$$

$$= \int_{\mathcal{X}\times\mathcal{Y}} L(y, \hat{f}(x))P(x,y)dxdy \qquad (1.27)$$

在这个表达式中：
- $R_{\exp}(\hat{f})$ 表示期望风险（泛化误差）
- $E_P$ 表示关于联合分布 $P(X,Y)$ 的期望
- $L(Y, \hat{f}(X))$ 是损失函数
- $\mathcal{X}$ 和 $\mathcal{Y}$ 分别表示输入空间和输出空间
- $P(x,y)$ 是 $X$ 和 $Y$ 的联合概率密度函数

### 生成模型与判别模型

1. 生成模型 (Generative Model):
   - 生成模型学习联合概率分布 P(X,Y)，然后求出条件概率分布 P(Y|X) 作为预测模型。
   - 它能够描述给定输入 X 产生输出 Y 的生成关系。
   - 生成模型的例子包括朴素贝叶斯法和隐马尔可夫模型。
   - 特点：
     - 可以还原出联合概率分布 P(X,Y)
     - 学习收敛速度较快，尤其是在样本容量增加时
     - 可以处理存在隐变量的情况

2. 判别模型 (Discriminative Model):
   - 判别模型直接学习决策函数 f(X) 或条件概率分布 P(Y|X)。
   - 它关注于对给定输入 X，应该预测什么样的输出 Y。
   - 判别模型的例子包括k近邻法、感知机、逻辑斯蒂回归模型、最大熵模型、支持向量机等。
   - 特点：
     - 直接面对预测，往往学习的精确率更高
     - 可以对数据进行各种程度的抽象，定义特征并使用特征
     - 可以简化学习问题

### 有监督学习应用

- 分类问题: $P(Y|X)$ or $Y=f(X)$
  - 分类情况: TP, FN, FP, TN
  - 性能指标: accuracy, precision, recall, f1
- 标注问题: $P(Y^{1},Y^{2},\cdots,Y^{n})|X^{1},X^{2},\cdots,X^{n}$
  - 指标和分类问题类似
  - 常用的方法：隐马尔可夫模型，条件随机场
- 回归问题: $Y=f(X)$ 等价于函数拟合，选择一条函数曲线使其很好地拟合已知数据且很好地预测未知数据
  - 常用损失函数：平凡损失函数；问题可以由由最小二乘法求解

## 感知机 Perceptron

- 在 1957 年由 Rosenblatt 提出
- 二分类线性分类模型，输入为特征向量，输出为实例的类别
- 感知机学习将实例划分为正负两类的线性划分的分离超平面，属于判别模型
- 使用基于误分类的损失函数，梯度下降法对损失函数最小化

### 感知机模型

假设输入空间（特征空间）是 $X \subseteq R^n$, 输出空间是 $Y=\{+1,-1\}$. 输入 $x \in X$ 表示实例的特征向量,对应于输入空间（特征空间）的点;输出 $y \in Y$ 表示实例的类别. 由输入空间到输出空间的如下函数

$$
f(x) = \text{sign}(w \cdot x + b)
$$

称为感知机. 其中, $w$ 和 $b$ 为感知机模型参数, $w \in R^n$ 叫作权值(weight)或权值向量(weight vector), $b \in R$ 叫作偏置(bias), $w \cdot x$ 表示 $w$ 和 $x$ 的内积. sign 是符号函数, 即

$$
\text{sign}(x) = \begin{cases}
+1, & x \geq 0 \\
-1, & x < 0
\end{cases}
$$

感知机模型的假设空间是特征空间中所有的线性分类模型(linear classification model)，即函数集合 $\{f|f(x)=w\cdot x+b\}$

感知机的集合解释：线性方程 $w\cdot x+b=0$ 对应于特征空间 $R^n$ 中的一个超平面(分离超平面 separating hyperplane) $S$ 其中 $w$ 是超平面的法向量 $b$ 是截距，空间划分为两个部分分别为正负类。

### 感知机学习策略

- 数据集线性可分: 存在某个超平面，完全正确划分两类到超平面两侧

# 无监督学习

# 深度学习
